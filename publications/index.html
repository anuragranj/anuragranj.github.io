<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Anurag Ranjan | publications</title>
  <meta name="description" content="Anurag's personal and research website.
">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/publications/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <a href="">
        <strong>Anurag</strong> Ranjan </a>
    </span>
    

    <nav class="site-nav">

      <div class="trigger">
        <!-- About -->
        <!--a class="page-link" href="/">about</a-->

        <!-- Blog -->
        <a class="page-link" href="/blog/">blog</a>

        <!-- Pages -->
        
          
        
          
            <a class="page-link" href="/bio/">bio</a>
          
        
          
            <a class="page-link" href="/media/">media</a>
          
        
          
            <a class="page-link" href="/projects/">projects</a>
          
        
          
            <a class="page-link" href="/publications/">publications</a>
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <h5 class="post-description">Publications by categories in reversed chronological order.</h5>
  </header>

  <article class="post-content publications clearfix">
    
<p><!-- <h3 class="year">2022</h3> --></p>
<ol class="bibliography"><li>

<div id="jiang2022neuman">
  <div class="col two">
  
    <span class="title">NeuMan: Neural Human Radiance Field from a Single Video</span>
    <span class="author">
      
        
          
            
              Wei Jiang,
            
          
        
      
        
          
            
              Kwang Moo Yi,
            
          
        
      
        
          
            
              Golnoosh Samei,
            
          
        
      
        
          
            
              Oncel Tuzel,
            
          
        
      
        
          and
          
            <em>Anurag Ranjan</em>
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the European conference on computer vision (ECCV)</em>
    
    
      2022
    
    </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2203.12575" target="_blank">arXiv</a>]
  
  
  
  
  
  
  
    [<a href="https://github.com/apple/ml-neuman" target="_blank">Code</a>]
  
  
    [<a href="https://machinelearning.apple.com/research/neural-human-radiance-field" target="_blank">URL</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <div>
    <p> NeuMan is a NeRF representation of human together with the scene. From a single clip (&lt;100 frames), NeuMan can perform view synthesis of the scene without/with the human in novel poses.</p>
  </div>
  

</div>

  
    <img class="col one" src="/assets/img/neuman_crop.gif" />
  

</div>
</li>
<li>

<div id="mobileone2022">
  <div class="col two">
  
    <span class="title">An Improved One millisecond Mobile Backbone</span>
    <span class="author">
      
        
          
            
              Pavan Kumar Anasosalu Vasu,
            
          
        
      
        
          
            
              James Gabriel,
            
          
        
      
        
          
            
              Jeff Zhu,
            
          
        
      
        
          
            
              Oncel Tuzel,
            
          
        
      
        
          and
          
            <em>Anurag Ranjan</em>
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:2206.04040</em>
    
    
      2022
    
    </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2206.04040" target="_blank">arXiv</a>]
  
  
  
  
  
  
  
    [<a href="https://github.com/apple/ml-mobileone" target="_blank">Code</a>]
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <div>
    <p>Fastest neural architecture on iPhone, runs under 1 ms.</p>
  </div>
  

</div>

  
    <img class="col one" src="https://raw.githubusercontent.com/apple/ml-mobileone/main/docs/accuracy_v_latency.jpg" />
  

</div>
</li>
<li>

<div id="nunez2021lcs">
  <div class="col two">
  
    <span class="title">LCS: Learning Compressible Subspaces for Adaptive Network Compression at Inference Time</span>
    <span class="author">
      
        
          
            
              Elvis Nunez,
            
          
        
      
        
          
            
              Maxwell Horton,
            
          
        
      
        
          
            
              Anish Prabhu,
            
          
        
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          
            
              Ali Farhadi,
            
          
        
      
        
          and
          
            
              Mohammad, Rastegari
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>
    
    
      2022
    
    </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2110.04252" target="_blank">arXiv</a>]
  
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <div>
    <p>LCS is a method for training a "compressible subspace" of neural networks that contains a fine-grained spectrum of models that range from highly efficient to highly accurate. These models require no retraining, thus the subspace of models can be deployed entirely on-device to allow adaptive network compression at inference time.</p>
  </div>
  

</div>

  
    <img class="col one" src="/assets/img/lcs.jpg" />
  

</div>
</li>
<li>

<div id="marin2021token">
  <div class="col two">
  
    <span class="title">Token Pooling in Vision Transformers</span>
    <span class="author">
      
        
          
            
              Dmitrii Marin,
            
          
        
      
        
          
            
              Jen-Hao Rick Chang,
            
          
        
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          
            
              Anish Prabhu,
            
          
        
      
        
          
            
              Mohammad Rastegari,
            
          
        
      
        
          and
          
            
              Oncel, Tuzel
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>
    
    
      2022
    
    </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2110.03860" target="_blank">arXiv</a>]
  
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <div>
    <p>Token Pooling is a novel downsampling operator for vision transformers that efficiently exploits redundancies in the images and intermediate token representations. Applied to DeiT, it achieves the same ImageNet top-1 accuracy using 42% fewer computations.</p>
  </div>
  

</div>

  
    <img class="col one" src="/assets/img/token_pooling.jpg" />
  

</div>
</li>
<li>

<div id="spin_eccv22">
  <div class="col two">
  
    <span class="title">SPIN: An Empirical Evaluation on Sharing Parameters of Isotropic Networks</span>
    <span class="author">
      
        
          
            
              Chien-Yu Lin,
            
          
        
      
        
          
            
              Anish Prabhu,
            
          
        
      
        
          
            
              Thomas Merth,
            
          
        
      
        
          
            
              Sachin Mehta,
            
          
        
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          
            
              Maxwell Horton,
            
          
        
      
        
          and
          
            
              Mohammad, Rastegari
            
          
        
      
    </span>

    <span class="periodical">
    
      <em></em>
    
    
      2022
    
    </span>
  

  <span class="links">
  <!--  -->
  
    [<a href="http://arxiv.org/abs/2207.10237" target="_blank">arXiv</a>]
  
  
  
  
  
  
  
    [<a href="https://github.com/apple/ml-spin" target="_blank">Code</a>]
  
  
  
  </span>

  <!-- Hidden abstract block -->
  

</div>

  

</div>
</li>
<li>

<div id="mittal2022naturalistic">
  <div class="col two">
  
    <span class="title">Naturalistic Head Motion Generation from Speech</span>
    <span class="author">
      
        
          
            
              Trisha Mittal,
            
          
        
      
        
          
            
              Zakaria Aldeneh,
            
          
        
      
        
          
            
              Masha Fedzechkina,
            
          
        
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          and
          
            
              Barry-John, Theobald
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:2210.14800</em>
    
    
      2022
    
    </span>
  

  <span class="links">
  <!--  -->
  
    [<a href="http://arxiv.org/abs/arXiv:2210.14800" target="_blank">arXiv</a>]
  
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  

</div>

  

</div>
</li></ol>

<p><!-- <h3 class="year">2021</h3> --></p>
<ol class="bibliography"><li>

<div id="roberts2021hypersim">
  <div class="col two">
  
    <span class="title">Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding</span>
    <span class="author">
      
        
          
            
              Mike Roberts,
            
          
        
      
        
          
            
              Jason Ramapuram,
            
          
        
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          
            
              Atulit Kumar,
            
          
        
      
        
          
            
              Miguel Angel Bautista,
            
          
        
      
        
          
            
              Nathan Paczan,
            
          
        
      
        
          
            
              Russ Webb,
            
          
        
      
        
          and
          
            
              Joshua M, Susskind
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision</em>
    
    
      2021
    
    </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2011.02523" target="_blank">arXiv</a>]
  
  
  
  
  
  
  
    [<a href="https://github.com/apple/ml-hypersim" target="_blank">Code</a>]
  
  
    [<a href="https://machinelearning.apple.com/research/hypersim" target="_blank">URL</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <div>
    <p>Hypersim is a photorealistic synthetic dataset for holistic indoor scene understanding containing 77,400 images of 461 indoor scenes with detailed per-pixel labels and corresponding ground truth geometry.</p>
  </div>
  

</div>

  
    <img class="col one" src="/assets/img/hypersim.jpg" />
  

</div>
</li></ol>

<p><!-- <h3 class="year">2020</h3> --></p>
<ol class="bibliography"><li>

<div id="rruiz2020morphgan">
  <div class="col two">
  
    <span class="title">MorphGAN: One-Shot Face Synthesis GAN for Detecting Recognition Bias</span>
    <span class="author">
      
        
          
            
              Nataniel Ruiz,
            
          
        
      
        
          
            
              Barry-John Theobald,
            
          
        
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          
            
              Ahmed Hussein Abdelaziz,
            
          
        
      
        
          and
          
            
              Nicholas, Apostoloff
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In arXiv</em>
    
    
      2020
    
    </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2012.05225" target="_blank">arXiv</a>]
  
  
  
  
  
  
  
  
    [<a href="https://machinelearning.apple.com/research/morphgan" target="_blank">URL</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <div>
    <p>MorphGAN can animate any face and control using a 3D rig. It’s one-shot and generalizes to in-the-wild unseen faces.</p>
  </div>
  

</div>

  
    <img class="col one" src="/assets/img/morphgan.gif" />
  

</div>
</li>
<li>

<div id="GIF2020">
  <div class="col two">
  
    <span class="title">GIF: Generative Interpretable Faces</span>
    <span class="author">
      
        
          
            
              Partha Ghosh,
            
          
        
      
        
          
            
              Pravir Singh Gupta,
            
          
        
      
        
          
            
              Roy Uziel,
            
          
        
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          
            
              <a href="https://ps.is.mpg.de/~black" target="_blank">Michael J. Black</a>,
            
          
        
      
        
          and
          
            
              <a href="https://sites.google.com/site/bolkartt/" target="_blank">Timo Bolkart</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In International Conference on 3D Vision (3DV)</em>
    
    
      2020
    
    </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/2009.00149" target="_blank">arXiv</a>]
  
  
  
  
  
  
  
    [<a href="https://github.com/ParthaEth/GIF" target="_blank">Code</a>]
  
  
    [<a href="http://gif.is.tue.mpg.de/" target="_blank">URL</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <div>
    <p>GIF generates realistic face images and animate them with a 3D face rig.</p>
  </div>
  

</div>

  
    <img class="col one" src="/assets/img/gif.png" />
  

</div>
</li>
<li>

<div id="ranjan2020learning">
  <div class="col two">
  
    <span class="title">Learning Multi-Human Optical Flow</span>
    <span class="author">
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          
            
              David T Hoffmann,
            
          
        
      
        
          
            
              <a href="https://www.dimtzionas.com/" target="_blank">Dimitrios Tzionas</a>,
            
          
        
      
        
          
            
              <a href="https://ps.is.tuebingen.mpg.de/person/stang" target="_blank">Siyu Tang</a>,
            
          
        
      
        
          
            
              <a href="https://ps.is.tuebingen.mpg.de/person/jromero" target="_blank">Javier Romero</a>,
            
          
        
      
        
          and
          
            
              <a href="https://ps.is.mpg.de/~black" target="_blank">Michael J Black</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>International Journal of Computer Vision</em>
    
    
      2020
    
    </span>
  

  <span class="links">
  <!--  -->
  
    [<a href="http://arxiv.org/abs/1910.11667" target="_blank">arXiv</a>]
  
  
  
  
  
  
  
  
    [<a href="http://humanflow.is.tue.mpg.de/" target="_blank">URL</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  

</div>

  
    <img class="col one" src="/assets/img/multihumanflow.jpg" />
  

</div>
</li>
<li>

<div id="ma2020dressing">
  <div class="col two">
  
    <span class="title">Learning to Dress 3D People in Generative Clothing</span>
    <span class="author">
      
        
          
            
              <a href="https://www.is.mpg.de/person/qma" target="_blank">Qianli Ma</a>,
            
          
        
      
        
          
            
              Jinlong Yang,
            
          
        
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          
            
              Sergi Pujades,
            
          
        
      
        
          
            
              Gerard Pons-Moll,
            
          
        
      
        
          
            
              <a href="https://ps.is.tuebingen.mpg.de/person/stang" target="_blank">Siyu Tang</a>,
            
          
        
      
        
          and
          
            
              <a href="https://ps.is.mpg.de/~black" target="_blank">Michael J. Black</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Computer Vision and Pattern Recognition (CVPR)</em>
    
    
      2020
    
    </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1907.13615" target="_blank">arXiv</a>]
  
  
  
  
  
  
  
    [<a href="https://github.com/QianliM/CAPE" target="_blank">Code</a>]
  
  
    [<a href="https://cape.is.tue.mpg.de/" target="_blank">URL</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <div>
    <p>CAPE is a Graph-CNN based generative model for dressing 3D meshes of human body. It is compatible with the popular body model, SMPL, and can generalize to diverse body shapes and body poses. It is designed to be "plug-and-play" for many applications that already use SMPL. The CAPE Dataset provides SMPL mesh registration of 4D scans of people in clothing, along with registered scans of the ground truth body shapes under clothing.</p>
  </div>
  

</div>

  
    <img class="col one" src="/assets/img/autoenclother.jpg" />
  

</div>
</li></ol>

<p><!-- <h3 class="year">2019</h3> --></p>
<ol class="bibliography"><li>

<div id="ranjan2019towards">
  <div class="col two">
  
    <h4> Towards Geometric Understanding of Motion </h4>
    <span> Thesis. University of Tuebingen Tuebingen. 2019. </span>
  

  <span class="links">
  <!--  -->
  
  
  
  
  
  
  
  
    [<a href="https://ps.is.tuebingen.mpg.de/publications/ranjan-thesis" target="_blank">URL</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  

</div>

  
    <img class="col one" src="https://ps.is.tuebingen.mpg.de/uploads/publication/image/22831/phdteaser.png" />
  

</div>
</li>
<li>

<div id="ranjan2019attacking">
  <div class="col two">
  
    <span class="title">Attacking Optical Flow</span>
    <span class="author">
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          
            
              <a href="https://avg.is.tuebingen.mpg.de/person/jjanai" target="_blank">Joel Janai</a>,
            
          
        
      
        
          
            
              <a href="http://www.cvlibs.net/" target="_blank">Andreas Geiger</a>,
            
          
        
      
        
          and
          
            
              <a href="https://ps.is.mpg.de/~black" target="_blank">Michael J Black</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In International Conference on Computer Vision (ICCV)</em>
    
    
      2019
    
    </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1910.10053" target="_blank">arXiv</a>]
  
  
  
  
  
  
  
    [<a href="https://github.com/anuragranj/flowattack" target="_blank">Code</a>]
  
  
    [<a href="https://flowattack.is.tuebingen.mpg.de/" target="_blank">URL</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <div>
    <p>Deep learning based optical flow methods are vulnerable to adversarial attacks. We show that it is very easy to attack these systems in real world by just placing a small printed patch in the scene.</p>
  </div>
  

</div>

  
    <img class="col one" src="/assets/img/attacking-OF-patch.png" />
  

</div>
</li>
<li>

<div id="ranjan2019competitive">
  <div class="col two">
  
    <span class="title">Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation</span>
    <span class="author">
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          
            
              <a href="https://varunjampani.github.io/" target="_blank">Varun Jampani</a>,
            
          
        
      
        
          
            
              Lukas Balles,
            
          
        
      
        
          
            
              Kihwan Kim,
            
          
        
      
        
          
            
              Deqing Sun,
            
          
        
      
        
          
            
              <a href="https://ps.is.tuebingen.mpg.de/person/jwulff" target="_blank">Jonas Wulff</a>,
            
          
        
      
        
          and
          
            
              <a href="https://ps.is.mpg.de/~black" target="_blank">Michael J Black</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>
    
    
      2019
    
    </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1805.09806" target="_blank">arXiv</a>]
  
  
  
  
  
  
  
    [<a href="https://github.com/anuragranj/cc" target="_blank">Code</a>]
  
  
  
    [<a href="https://developer.nvidia.com/gtc/2019/video/S9575" target="_blank">Talk</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <div>
    <p>Unsupervised learning of several interconnected problems in low-level vision: single view depth prediction, camera motion estimation, optical flow and segmentation of a video into the static scene and moving regions.</p>
  </div>
  

</div>

  
    <img class="col one" src="/assets/img/joint_teaser.png" />
  

</div>
</li>
<li>

<div id="ray2019unsupervised">
  <div class="col two">
  
    <span class="title">Unsupervised video segmentation</span>
    <span class="author">
      
        
          
            
              Benjamin Ray,
            
          
        
      
        
          and
          
            <em>Anurag Ranjan</em>
          
        
      
    </span>

    <span class="periodical">
    
    
      2019
    
    </span>
  

  <span class="links">
  <!--  -->
  
  
  
  
  
  
  
  
    [<a href="https://patents.google.com/patent/US10402986B2/en" target="_blank">URL</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  

</div>

  

</div>
</li>
<li>

<div id="cudeiro2019capture">
  <div class="col two">
  
    <span class="title">Capture, Learning, and Synthesis of 3D Speaking Styles</span>
    <span class="author">
      
        
          
            
              Daniel Cudeiro,
            
          
        
      
        
          
            
              <a href="https://sites.google.com/site/bolkartt/" target="_blank">Timo Bolkart</a>,
            
          
        
      
        
          
            
              Cassidy Laidlaw,
            
          
        
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          and
          
            
              <a href="https://ps.is.mpg.de/~black" target="_blank">Michael J Black</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>
    
    
      2019
    
    </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1905.03079" target="_blank">arXiv</a>]
  
  
  
  
  
  
  
    [<a href="https://github.com/TimoBolkart/voca" target="_blank">Code</a>]
  
  
    [<a href="https://voca.is.tue.mpg.de/" target="_blank">URL</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <div>
    <p>A neural network for generating 3D facial motion by using raw speech audio. Works on a veriety of unseen faces.</p>
  </div>
  

</div>

  
    <img class="col one" src="/assets/img/voca.png" />
  

</div>
</li></ol>

<p><!-- <h3 class="year">2018</h3> --></p>
<ol class="bibliography"><li>

<div id="ranjan2018generating">
  <div class="col two">
  
    <span class="title">Generating 3D faces using convolutional mesh autoencoders</span>
    <span class="author">
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          
            
              <a href="https://sites.google.com/site/bolkartt/" target="_blank">Timo Bolkart</a>,
            
          
        
      
        
          
            
              <a href="https://ps.is.mpg.de/~ssanyal" target="_blank">Soubhik Sanyal</a>,
            
          
        
      
        
          and
          
            
              <a href="https://ps.is.mpg.de/~black" target="_blank">Michael J Black</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In European Conference on Computer Vision (ECCV)</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1807.10267" target="_blank">arXiv</a>]
  
  
  
  
  
  
  
    [<a href="https://github.com/anuragranj/coma" target="_blank">Code</a>]
  
  
    [<a href="http://coma.is.tue.mpg.de/" target="_blank">URL</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <div>
    <p>A non-linear model for generating 3D faces using a Convolutional Autoencoder that operates directly on meshes. Our model is state of the art in generating diverse range of 3D facial meshes.</p>
  </div>
  

</div>

  
    <img class="col one" src="/assets/img/coma_samples.jpg" />
  

</div>
</li>
<li>

<div id="ranjan2018learning">
  <div class="col two">
  
    <span class="title">Learning human optical flow</span>
    <span class="author">
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          
            
              <a href="https://ps.is.tuebingen.mpg.de/person/jromero" target="_blank">Javier Romero</a>,
            
          
        
      
        
          and
          
            
              <a href="https://ps.is.mpg.de/~black" target="_blank">Michael J Black</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>British Machine Vision Conference (BMVC)</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
    [<a href="http://arxiv.org/abs/1806.05666" target="_blank">arXiv</a>]
  
  
  
  
  
  
  
    [<a href="https://github.com/anuragranj/humanflow" target="_blank">Code</a>]
  
  
    [<a href="http://humanflow.is.tue.mpg.de/" target="_blank">URL</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <div>
    <p>Learning optical flow for humans is difficult. So, we created a synthetic dataset with realistic humans and trained a neural network on it.</p>
  </div>
  

</div>

  
    <img class="col one" src="/assets/img/humanflow.png" />
  

</div>
</li>
<li>

<div id="janai2018unsupervised">
  <div class="col two">
  
    <span class="title">Unsupervised learning of multi-frame optical flow with occlusions</span>
    <span class="author">
      
        
          
            
              <a href="https://avg.is.tuebingen.mpg.de/person/jjanai" target="_blank">Joel Janai</a>,
            
          
        
      
        
          
            
              Fatma Guney,
            
          
        
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          
            
              <a href="https://ps.is.mpg.de/~black" target="_blank">Michael Black</a>,
            
          
        
      
        
          and
          
            
              <a href="http://www.cvlibs.net/" target="_blank">Andreas Geiger</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In European Conference on Computer Vision (ECCV)</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  
    [<a href="/assets/pdf/Janai2018ECCV.pdf" target="_blank">PDF</a>]
  
  
  
  
  
    [<a href="https://github.com/anuragranj/back2future.pytorch" target="_blank">Code</a>]
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <div>
    <p>We propose a framework for unsupervised learning of optical flow and occlusions over multiple frames. Our multi-frame, occlusion-sensitive formulation outperforms existing unsupervised two-frame methods and even produces results on par with some fully supervised methods.</p>
  </div>
  

</div>

  

</div>
</li></ol>

<p><!-- <h3 class="year">2017</h3> --></p>
<ol class="bibliography"><li>

<div id="ranjan2017optical">
  <div class="col two">
  
    <span class="title">Optical flow estimation using a spatial pyramid network</span>
    <span class="author">
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          and
          
            
              <a href="https://ps.is.mpg.de/~black" target="_blank">Michael J Black</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>
    
    
      2017
    
    </span>
  

  <span class="links">
  <!--  -->
  
    [<a href="http://arxiv.org/abs/1611.00850" target="_blank">arXiv</a>]
  
  
  
  
  
  
  
    [<a href="https://github.com/anuragranj/spynet" target="_blank">Code</a>]
  
  
    [<a href="http://spynet.is.tue.mpg.de/" target="_blank">URL</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  

</div>

  
    <img class="col one" src="/assets/img/sintel_pyramid.png" />
  

</div>
</li>
<li>

<div id="neog2017seeing">
  <div class="col two">
  
    <span class="title">Seeing Skin in Reduced Coordinates</span>
    <span class="author">
      
        
          
            
              <a href="https://debanga.github.io/" target="_blank">Debanga R Neog</a>,
            
          
        
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          and
          
            
              <a href="https://sensorimotor.cs.ubc.ca/pai/" target="_blank">Dinesh K Pai</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG)</em>
    
    
      2017
    
    </span>
  

  <span class="links">
  <!--  -->
  
  
  
  
  
  
  
  
    [<a href="http://www.cs.ubc.ca/research/seeingskininreducedcoordinates/" target="_blank">URL</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  

</div>

  

</div>
</li></ol>

<p><!-- <h3 class="year">2016</h3> --></p>
<ol class="bibliography"><li>

<div id="neog2016interactive">
  <div class="col two">
  
    <span class="title">Interactive gaze driven animation of the eye region</span>
    <span class="author">
      
        
          
            
              <a href="https://debanga.github.io/" target="_blank">Debanga R Neog</a>,
            
          
        
      
        
          
            
              João L Cardoso,
            
          
        
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          and
          
            
              <a href="https://sensorimotor.cs.ubc.ca/pai/" target="_blank">Dinesh K Pai</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In International Conference on Web3D Technology</em>
    
    
      2016
    
    </span>
  

  <span class="links">
  <!--  -->
  
  
  
  
  
  
  
  
    [<a href="http://www.cs.ubc.ca/research/eyemoveweb3d16/" target="_blank">URL</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  

</div>

  
    <img class="col one" src="/assets/img/faces.jpg" />
  

</div>
</li></ol>

<p><!-- <h3 class="year">2015</h3> --></p>
<ol class="bibliography"><li>

<div id="ranjan2015learning">
  <div class="col two">
  
    <h4> Learning periorbital soft tissue motion </h4>
    <span> Thesis. University of British Columbia,. 2015. </span>
  

  <span class="links">
  <!-- 
    [<a class="abstract">Abs</a>]
   -->
  
  
  
  
  
  
  
  
    [<a href="https://open.library.ubc.ca/cIRcle/collections/ubctheses/24/items/1.0166703" target="_blank">URL</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <div>
    <p>We model the soft tissues around the eyes that are associated with subtle and fast motions and convey emotions during facial expressions. Our data driven model that can efficiently learn and reproduce the complex motion of these periorbital soft tissues.</p>
  </div>
  

</div>

  

</div>
</li>
<li>

<div id="neog2015gaze">
  <div class="col two">
  
    <span class="title">Gaze driven animation of eyes</span>
    <span class="author">
      
        
          
            
              <a href="https://debanga.github.io/" target="_blank">Debanga Raj Neog</a>,
            
          
        
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          
            
              João L Cardoso,
            
          
        
      
        
          and
          
            
              <a href="https://sensorimotor.cs.ubc.ca/pai/" target="_blank">Dinesh K Pai</a>
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In ACM SIGGRAPH/Eurographics Symposium on Computer Animation</em>
    
    
      2015
    
    </span>
  

  <span class="links">
  <!--  -->
  
  
  
    [<a href="/assets/pdf/Gaze_driven_animation.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  

</div>

  

</div>
</li></ol>

<p><!-- <h3 class="year">2012</h3> --></p>
<ol class="bibliography"><li>

<div id="bongale2012implementation">
  <div class="col two">
  
    <span class="title">Implementation of 3D object recognition and tracking</span>
    <span class="author">
      
        
          
            
              Pankaj Bongale,
            
          
        
      
        
          
            <em>Anurag Ranjan</em>,
          
        
      
        
          and
          
            
              Sahil, Anand
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In International conference on Recent Advances in Computing and Software Systems (RACSS)</em>
    
    
      2012
    
    </span>
  

  <span class="links">
  <!--  -->
  
  
  
    [<a href="/assets/pdf/bongale2012implementation.pdf" target="_blank">PDF</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  

</div>

  

</div>
</li></ol>


  </article>

  

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2023 Anurag Ranjan.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
<script src="/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/font-awesome.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXX-X', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
